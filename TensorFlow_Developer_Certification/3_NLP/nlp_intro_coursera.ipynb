{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTv2HlwP0uvCUseQ+fW5aR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"C28IiUSe5vAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word index"],"metadata":{"id":"Gmf0y2BrAzXR"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"BoPFp9tKbH9s","executionInfo":{"status":"ok","timestamp":1689531817198,"user_tz":-180,"elapsed":240,"user":{"displayName":"Endless Python","userId":"11704323545799837523"}},"outputId":"aae09dbf-003a-414e-b7ea-c82d494017db"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n","[[4, 2, 1, 3], [1, 3, 1]]\n"]}],"source":["#@title word index without \\<OOV\\>\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat',\n","    'You love my dog!',\n","    'Do you think my dog is amazing?'\n","    ]\n","\n","tokenizer = Tokenizer(num_words = 100)\n","tokenizer.fit_on_texts(sentences) # generate indices for each word in the corpus\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","print(word_index)\n","print(sequences)\n","\n","# if we use the same tokenizer on other text\n","other_sentences = [\n","    'I really love my dog',\n","    'my dog loves my manatee'\n","]\n","test_seq = tokenizer.texts_to_sequences(other_sentences)\n","print(test_seq)\n","# for some words indices were not generated bcz we fit tokenizer on other corpus of text"]},{"cell_type":"code","source":["#@title word index with \\<OOV\\>\n","tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n","\n","test_seq = tokenizer.texts_to_sequences(other_sentences)\n","print(test_seq)\n","# all words not in word index will be treated as OOV (out of vocabulary) token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"FYcKJXMa7NOJ","executionInfo":{"status":"ok","timestamp":1689532000282,"user_tz":-180,"elapsed":7,"user":{"displayName":"Endless Python","userId":"11704323545799837523"}},"outputId":"74e30a16-5308-47bf-bf03-3e96e532b23c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"]}]},{"cell_type":"markdown","source":["# Padding"],"metadata":{"id":"ba_D9MX7BI9d"}},{"cell_type":"markdown","source":["When we fed images into the network for training, we needed them to be uniform in size and used generators to resize the image to fit.  \n","There is similar requirement for text. Before we can train with text we need to have some uniformity of size."],"metadata":{"id":"HD5CtiLXBKdG"}},{"cell_type":"code","source":["padded = pad_sequences(sequences) # default padding='pre'\n","padded_custom_params = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n","print(word_index)\n","print(sequences)\n","print(padded)\n","print(padded_custom_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3anc5xe89Bf","executionInfo":{"status":"ok","timestamp":1689533731367,"user_tz":-180,"elapsed":233,"user":{"displayName":"Endless Python","userId":"11704323545799837523"}},"outputId":"3f17a1b5-e4ff-4ac5-a421-508e63cbcc4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n","[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n","[[ 0  0  0  4  2  1  3]\n"," [ 0  0  0  4  2  1  6]\n"," [ 0  0  0  5  2  1  3]\n"," [ 7  5  8  1  3  9 10]]\n","[[4 2 1 3 0]\n"," [4 2 1 6 0]\n"," [5 2 1 3 0]\n"," [7 5 8 1 3]]\n"]}]}]}